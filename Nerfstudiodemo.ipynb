{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arumajirou/-daily-test/blob/main/Nerfstudiodemo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SiiXJ7K_fePG"
      },
      "source": [
        "<p align=\"center\">\n",
        "    <picture>\n",
        "    <source media=\"(prefers-color-scheme: dark)\" srcset=\"https://docs.nerf.studio/en/latest/_images/logo-dark.png\">\n",
        "    <source media=\"(prefers-color-scheme: light)\" srcset=\"https://docs.nerf.studio/en/latest/_images/logo.png\">\n",
        "    <img alt=\"nerfstudio\" src=\"https://docs.nerf.studio/en/latest/_images/logo.png\" width=\"400\">\n",
        "    </picture>\n",
        "</p>\n",
        "\n",
        "\n",
        "# ネルフスタジオNeRF(新規視点の画像生成ネットワーク)のためのコラボレーション・フレンドリーなスタジオ\n",
        "\n",
        "\n",
        "![GitHub stars](https://img.shields.io/github/stars/NeRF(新規視点の画像生成ネットワーク)studio-project/nerfstudio?color=gold&style=social)\n",
        "\n",
        "ネルフスタジオNeRF(新規視点の画像生成ネットワーク)のためのコラボレーション・フレンドリーなスタジオ\n",
        "GitHub stars\n",
        "\n",
        "本コラボでは、NeRF(新規視点の画像生成ネットワーク)studioのNeRF(新規視点の画像生成ネットワーク)を既成のデータセットや自分のビデオ・画像から学習・閲覧する方法を紹介します。\n",
        "\n",
        "\n",
        "\n",
        "Google Colabのフォーマットについては、NeX社にクレジットをお願いしています\n",
        "\n",
        "\\\\\n",
        "\n",
        "Credit to [NeX](https://nex-mpi.github.io/) for Google Colab format."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **SDFStudio**\n",
        "SDFStudioは、ニューラル(神経)・インプリシット(潜在)・サーフェス(表面) 再構成(神経の潜在的な表面再構成)のための統一された機能の枠組みで、\n",
        "\n",
        "素晴らしいNeRF(新規視点の画像生成ネットワーク)studioプロジェクトの上に構築されています。\n",
        "\n",
        "私たちは、3つの主要な陰解法表面再構成法の統一的な実装を提供します。\n",
        "\n",
        "---\n",
        "**UniSurf(ユニサーフ)**\n",
        "\n",
        "UniSurfは、まず**サーフェス(表面)の交点**を求め、**サーフェス(表面)周辺の点を標本抽出**する。標本抽出範囲は**大きな範囲**から始まり、トレーニング中に徐々に**小さな範囲**に減少していく。あるレイに対して**サーフェス(表面)**が見つからない場合、UniSurfはレイの**near((空間・時間的に)近く)**と**far((空間・時間的に)遠く)**の値に従って一様に標本抽出します。\n",
        "\n",
        "---\n",
        "VolSDF\n",
        "\n",
        "VolSDFは**エラーバウンド(誤り限界)サンプラー(一定の方向に進ませるもの)**を用い、**SDF値(SignedDistanceFunction value（符号付距離関数値）)**を**密度値に変換**し、NeRF(新規視点の画像生成ネットワーク)のように通常の**ボリュームレンダリング(不透明度を変えることで空気感などの演出を行うことで自然で滑らかな濃淡を生み、リアルな表現を計算する表示法のこと)**を使用します。\n",
        "\n",
        "---\n",
        "**NeuS**\n",
        "**NeuS**は**多段階の階層的標本抽出**を用い、**シグモイド関数(S字型の滑らかな曲線で、「0」～「1」の間の値を返す、ニューラル(神経)ネットワークの活性化関数)に**基づいて**SDF値(SignedDistanceFunction value（符号付距離関数値）)**を**アルファ値(デジタル画像データにおいて色情報とは別に、画素（ピクセル）ごとに設けられた付加情報を表す数値である。特に透明度（不透明度）として利用されることが多い。)**に変換する。\n",
        "\n",
        "---\n",
        "3つの主要な陰解面再構成法の統一的な実装を提供します。\n",
        "また、\n",
        "\n",
        "---\n",
        "**MLP（（多層パーセプトロン）とは人間の脳をモデル化したパーセプトロンというものを多層化したもののこと）**\n",
        "3次元位置はNeRF(新規視点の画像生成ネットワーク)と同様に位置 エン コーディングを行い，多層パーセプトロン（MLP（（多層パーセプトロン）とは人間の脳をモデル化したパーセプトロンというものを多層化したもののこと））ネットワークに渡して，SDF値(SignedDistanceFunction value（符号付距離関数値）)，法線，形状特徴を予測させます．8層、512次元の隠れ次元を持つMLP（（多層パーセプトロン）とは人間の脳をモデル化したパーセプトロンというものを多層化したもののこと）でVolSDFを学習するには、以下のコマンドを実行します。\n",
        "\n",
        "---\n",
        "**Tri-plane**\n",
        "3次元位置はまず直交する3つの平面にマッピング(対応付け)され，バイリニア(縦と横の2（Bi）方向に対して線形（Linear((空間・時間的に)近く)）補間をすること)補間を用いて各平面の特徴ベクトル(大きさだけでなく、向きももった量)を取得し，MLP（（多層パーセプトロン）とは人間の脳をモデル化したパーセプトロンというものを多層化したもののこと）の入力として連結されます．\n",
        "\n",
        "---\n",
        "**Multi-res.feature grids**\n",
        "3次元位置は、まず、三次補間により、多重解像度特徴グリッド(方眼)にマッピング(対応付け)され、対応する特徴ベクトル(大きさだけでなく、向きももった量)を取得する。この特徴ベクトル(大きさだけでなく、向きももった量)をMLP（（多層パーセプトロン）とは人間の脳をモデル化したパーセプトロンというものを多層化したもののこと）の入力として使用し、SDF、法線、ジオメトリ(幾何学、形状)の各特徴を予測します。2つのレイヤーと256の隠れ次元を持つ多解像度特徴グリッド(方眼)表現によるVolSDFモデルをトレーニングする\n",
        "\n",
        "---\n",
        "\n",
        "といった様々なシーン表現、UniSurf(**サーフェス(表面)の交点**を求め、**サーフェス(表面)周辺の点を標本抽出**する。)のような表面誘導型標本抽出や\n",
        "\n",
        "NeuralReconW(神経偵察W)のボクセル(三次元表現などで、立体物の表現に用いられる小さな立方体の最小単位。二次元画像におけるピクセル)表面誘導型標本抽出といった複数の点標本抽出戦略を支援しています。\n",
        "\n",
        "さらに、\n",
        "単眼の手がかりの利用（MonoSDF）\n",
        "MonoSDFはVolSDFをベースに、**単眼の俯瞰**と**法線の手がかり**を**追加監督**として使用することを提案している。**疎な設定（視点が少ない）**や**室内シーン**で**特に有用**です。\n",
        "---\n",
        "\n",
        "幾何学的正則化（UniSurf）、\n",
        "\n",
        "幾何学的正則化（UniSurf）は、**マージ(結合)の交点**を求め、**マージ(結合)周辺の点**をまず標本抽出する。標本抽出範囲は**大きな範囲から始まり**、トレーニング中に徐々にあるレイに対して転送が適用されない場合、UniSurfはレイの**near((空間・時間的に)近く)**と**far((空間・時間的に)遠く)**の値に従います。\n",
        "\n",
        "---\n",
        "\n",
        "多視点整合性（Geo-NeuS）\n",
        "多視点整合性（Geo-NeuS）は NeuS(2次元画像入力から物体やシーンを忠実に 再構成する新しい神経表面再構成法、NeuS)に基づいて構築され、多視点測光一貫性損失を提案します。\n",
        "---\n",
        "\n",
        "など、この分野における最近の進歩も統合されています。\n",
        "\n",
        "SDFStudioは、統一された機能の実装により、ある手法から別の手法への観念の移行を容易にします。\n",
        "\n"
      ],
      "metadata": {
        "id": "hcDl4SP0Ks-k"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yyx5h6kz5ga7"
      },
      "source": [
        "# **よくある質問**\n",
        "* **カスタムデータのダウンロードが止まっている（出力されない）**\n",
        "*- Colabのバグです。\n",
        "データは処理されていますが、完了までに時間がかかる場合があります。data/nerfstudio/custom_data/transforms.jsonが存在すれば処理が完了したことになります。\n",
        "*- 早期にセルを終了させると、学習ができなくなります。\n",
        "* **カスタムデータの処理に時間がかかっている。**\n",
        "*- データの処理にかかる時間は、画像の枚数とその解像度によって異なります。\n",
        "*- 処理に時間がかかりすぎる場合は、カスタムデータの解像度を下げてみてください。\n",
        "- **エラーです。データ処理が完了しませんでした。**\n",
        "-* これは、データ処理スクリプトが完全に完了しなかったことを意味します。\n",
        "-*これは、画像の枚数が少ないか、画像の品質が低いことが原因である可能性があります。\n",
        "-* 処理に成功する可能性を高めるために、モーションブラーがほとんどなく、シーンの視覚的な重なりが多い画像をお勧めします。\n",
        "- **トレーニングの進捗が表示されない。**\n",
        "-* 出力されないのは、Colabのバグです。ビューアからトレーニングの進捗を確認することができます。\n",
        "- **ビューアの品質が悪い / 解像度が低い**\n",
        "*- これは、ビューアをレンダリングするために、トレーニングに多くのGPUが使用されていることが原因である可能性があります。\n",
        "*- トレーニングを一時停止するか、トレーニングの使用率を下げてみてください。\n",
        "- **警告: 'root'ユーザーとしてpipを実行しています...::**\n",
        "-* この警告とその他のpipの警告やエラーは、安全に無視してかまいません。\n",
        "-**その他の問題？**\n",
        "-* GitHub リポジトリにイシューを作成してください。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "RGr33zHaHak0"
      },
      "outputs": [],
      "source": [
        "#@markdown Condaのインストール(ランタイムの再起動が必要)</h1>\n",
        "\n",
        "!pip install -q condacolab\n",
        "import condacolab\n",
        "condacolab.install()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "9oyLHl8QfYwP"
      },
      "outputs": [],
      "source": [
        "#@markdown # Nerfstudioのインストール (約10分)\n",
        "\n",
        "%cd /content/\n",
        "!pip install --upgrade pip\n",
        "!pip install torch==1.12.1+cu113 torchvision==0.13.1+cu113 -f https://download.pytorch.org/whl/torch_stable.html\n",
        "\n",
        "# TinyCudaのインストール\n",
        "%cd /content/\n",
        "!gdown \"https://drive.google.com/u/1/uc?id=1q8fuc-Mqiev5GTBTRA5UPgCaQDzuqKqj\" \n",
        "!pip install tinycudann-1.6-cp37-cp37m-linux_x86_64.whl\n",
        "\n",
        "# COLMAPのインストール\n",
        "%cd /content/\n",
        "!conda install -c conda-forge colmap\n",
        "\n",
        "# nerfstudioのインストール\n",
        "%cd /content/\n",
        "# !pip install nerfstudio\n",
        "!pip install git+https://github.com/nerfstudio-project/nerfstudio.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "msVLprI4gRA4"
      },
      "outputs": [],
      "source": [
        "#@markdown データのダウンロードと加工\n",
        "#@markdown 前もって調整・定義された設置値のシーンを選ぶか、自分の画像や動画をアップロードする\n",
        "import os\n",
        "import glob\n",
        "from google.colab import files\n",
        "from IPython.core.display import display, HTML\n",
        "\n",
        "scene = '\\uD83D\\uDDBC poster' #@param ['🖼 poster', '🚜 dozer', '🌄 desolation', '📤 upload your images' , '🎥 upload your own video', '🔺 upload Polycam data', '💽 upload your own Record3D data']\n",
        "scene = ' '.join(scene.split(' ')[1:])\n",
        "\n",
        "if scene == \"upload Polycam data\":\n",
        "    %cd /content/\n",
        "    !mkdir -p /content/data/nerfstudio/custom_data\n",
        "    %cd /content/data/nerfstudio/custom_data/\n",
        "    uploaded = files.upload()\n",
        "    dir = os.getcwd()\n",
        "    if len(uploaded.keys()) > 1:\n",
        "        print(\"エラー、Polycam データの処理中に単一の .zip ファイルをアップロードしてください\")\n",
        "    dataset_dir = [os.path.join(dir, f) for f in uploaded.keys()][0]\n",
        "    !ns-process-data polycam --data $dataset_dir --output-dir /content/data/nerfstudio/custom_data/\n",
        "    scene = \"custom_data\"\n",
        "elif scene == 'upload your own Record3D data':\n",
        "    display(HTML('<h3>Zip your Record3D folder, and upload.</h3>'))\n",
        "    display(HTML('<h3>More information on Record3D can be found <a href=\"https://docs.nerf.studio/en/latest/quickstart/custom_dataset.html#record3d-capture\" target=\"_blank\">here</a>.</h3>'))\n",
        "    %cd /content/\n",
        "    !mkdir -p /content/data/nerfstudio/custom_data\n",
        "    %cd /content/data/nerfstudio/custom_data/\n",
        "    uploaded = files.upload()\n",
        "    dir = os.getcwd()\n",
        "    preupload_datasets = [os.path.join(dir, f) for f in uploaded.keys()]\n",
        "    record_3d_zipfile = preupload_datasets[0]\n",
        "    !unzip $record_3d_zipfile -d /content/data/nerfstudio/custom_data\n",
        "    custom_data_directory = glob.glob('/content/data/nerfstudio/custom_data/*')[0]\n",
        "    !ns-process-data record3d --data $custom_data_directory --output-dir /content/data/nerfstudio/custom_data/\n",
        "    scene = \"custom_data\"\n",
        "elif scene in ['upload your images', 'upload your own video']:\n",
        "    display(HTML('<h3>Select your custom data</h3>'))\n",
        "    display(HTML('<p/>You can select multiple images by pressing ctrl, cmd or shift and click.<p>'))\n",
        "    display(HTML('<p/>Note: This may take time, especially on hires inputs, so we recommend to download dataset after creation.<p>'))\n",
        "    !mkdir -p /content/data/nerfstudio/custom_data\n",
        "    if scene == 'upload your images':\n",
        "        !mkdir -p /content/data/nerfstudio/custom_data/raw_images\n",
        "        %cd /content/data/nerfstudio/custom_data/raw_images\n",
        "        uploaded = files.upload()\n",
        "        dir = os.getcwd()\n",
        "    else:\n",
        "        %cd /content/data/nerfstudio/custom_data/\n",
        "        uploaded = files.upload()\n",
        "        dir = os.getcwd()\n",
        "    preupload_datasets = [os.path.join(dir, f) for f in uploaded.keys()]\n",
        "    del uploaded\n",
        "    %cd /content/\n",
        "\n",
        "    if scene == 'upload your images':\n",
        "        !ns-process-data images --data /content/data/nerfstudio/custom_data/raw_images --output-dir /content/data/nerfstudio/custom_data/\n",
        "    else:\n",
        "        video_path = preupload_datasets[0]\n",
        "        !ns-process-data video --data $video_path --output-dir /content/data/nerfstudio/custom_data/\n",
        "\n",
        "    scene = \"custom_data\"\n",
        "else:\n",
        "    %cd /content/\n",
        "    !ns-download-data nerfstudio --capture-name=$scene\n",
        "\n",
        "print(\"データ処理成功!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "VoKDxqEcjmfC"
      },
      "outputs": [],
      "source": [
        "#@markdown 観測者の設定と起動\n",
        "\n",
        "%cd /content\n",
        "\n",
        "# localtunnelのインストール\n",
        "# localtunnel https://github.com/localtunnel/localtunnelを使用していますが、ngrokも使用可能です。\n",
        "!npm install -g localtunnel\n",
        "\n",
        "# デフォルトであるトンネルポート7007\n",
        "!rm url.txt 2> /dev/null\n",
        "get_ipython().system_raw('lt --port 7007 >> url.txt 2>&1 &')\n",
        "\n",
        "import time\n",
        "time.sleep(3) #前のコマンドは url.txt に書き込む時間が必要です。\n",
        "\n",
        "\n",
        "with open('url.txt') as f:\n",
        "  lines = f.readlines()\n",
        "websocket_url = lines[0].split(\": \")[1].strip().replace(\"https\", \"wss\")\n",
        "# from nerfstudio.utils.io import load_from_json\n",
        "# from pathlib import Path\n",
        "# json_filename = \"nerfstudio/nerfstudio/viewer/app/package.json\"\n",
        "# version = load_from_json(Path(json_filename))[\"version\"]\n",
        "url = f\"https://viewer.nerf.studio/?websocket_url={websocket_url}\"\n",
        "print(url)\n",
        "print(\"トレーニングを開始したら、ページの更新をクリックする必要があるかもしれません！\")\n",
        "from IPython import display\n",
        "display.IFrame(src=url, height=800, width=\"100%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "m_N8_cLfjoXD"
      },
      "outputs": [],
      "source": [
        "#@markdown トレーニング開始\n",
        "\n",
        "%cd /content\n",
        "if os.path.exists(f\"data/nerfstudio/{scene}/transforms.json\"):\n",
        "    !ns-train nerfacto --viewer.websocket-port 7007 nerfstudio-data --data data/nerfstudio/$scene --downscale-factor 4\n",
        "else:\n",
        "    display(HTML('<h3 style=\"color:red\">Error: Data processing did not complete</h3>'))\n",
        "    display(HTML('<h3>Please re-run `Downloading and Processing Data`, or view the FAQ for more info.</h3>'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "WGt8ukG6Htg3"
      },
      "outputs": [],
      "source": [
        "#@title # ビデオをレンダリング { vertical-output:true } { vertical-output: true }\n",
        "#@markdown 視点内からカメラパスをエクスポート(特定のアプリケーションで読み込むことができるようなファイルの形式に出力する作業)し、このセルを実行します\n",
        "#@markdown レンダリングされた動画は、renders/output.mp4 にあるはずです。\n",
        "\n",
        "\n",
        "base_dir = \"/content/outputs/data-nerfstudio-\" + scene + \"/nerfacto/\"\n",
        "training_run_dir = base_dir + os.listdir(base_dir)[0]\n",
        "\n",
        "from IPython.core.display import display, HTML\n",
        "display(HTML('<h3>Upload the camera path JSON.</h3>'))\n",
        "%cd $training_run_dir\n",
        "uploaded = files.upload()\n",
        "uploaded_camera_path_filename = list(uploaded.keys())[0]\n",
        "\n",
        "config_filename = training_run_dir + \"/config.yml\"\n",
        "camera_path_filename = training_run_dir + \"/\" + uploaded_camera_path_filename\n",
        "camera_path_filename = camera_path_filename.replace(\" \", \"\\\\ \").replace(\"(\", \"\\\\(\").replace(\")\", \"\\\\)\")\n",
        "\n",
        "%cd /content/\n",
        "!ns-render --load-config $config_filename --traj filename --camera-path-filename $camera_path_filename --output-path renders/output.mp4"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "w7UV5j5mJPSi"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.8.13 ('nerfstudio')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "c59f626636933ef1dc834fb3684b382f705301c5306cf8436d2da634c2289783"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}